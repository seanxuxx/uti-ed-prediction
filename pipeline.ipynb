{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchsummaryX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import zipfile\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (LabelEncoder, MinMaxScaler, OneHotEncoder,\n",
    "                                   OrdinalEncoder, StandardScaler)\n",
    "from sklearn.svm import SVC\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    DEVICE_N_WORKERS = 4\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    DEVICE_N_WORKERS = 0\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'init_lr': 5e-4,\n",
    "    'dropout_rate': 0.2,\n",
    "    'scheduler_factor': 0.8,\n",
    "    'scheduler_patience': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = os.path.join(os.getcwd(), 'data', 'S1File.csv')\n",
    "metadata_filename = os.path.join(os.getcwd(), 'data', 'metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_filename)\n",
    "metadata = pd.read_csv(metadata_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = metadata.variable.to_list()\n",
    "label = 'UCX_abnormal'  # UCX test result\n",
    "diagnosis = 'UTI_diag'  # ED diagnosis\n",
    "\n",
    "# Map UCX and clinical diagnosis to int\n",
    "df[label] = df[label].map({'yes': 1, 'no': 0})\n",
    "df[diagnosis] = df[diagnosis].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Reorder columns\n",
    "df = df[[label] + [diagnosis] + features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    First, drop the columns with not_reported values > 10%\n",
    "    Then, drop observations with not_reported or other values\n",
    "    return cleaned dataframe\n",
    "    \"\"\"\n",
    "    # Drop the columns with not_reported values > 10%\n",
    "    drop = []\n",
    "    demo = ['age', 'gender', 'race', 'ethnicity', 'lang',\n",
    "            'employStatus', 'maritalStatus', 'chief_complaint']\n",
    "    cols = [i for i in df.columns if i not in demo]\n",
    "    for col in cols:\n",
    "        ratio = df[col][df[col] == 'not_reported'].count()/df.shape[0]*100\n",
    "        if ratio > 0.1:\n",
    "            drop.append(col)\n",
    "    df = df.drop(labels=drop, axis=1)\n",
    "\n",
    "    # Drop observations with not_reported or other values\n",
    "    df= df[~df.apply(lambda row: row =='not_reported').any(axis=1)]\n",
    "    df= df[~df.apply(lambda row: row =='other').any(axis=1)]\n",
    "    df= df[~df.apply(lambda row: row =='4+').any(axis=1)]\n",
    "\n",
    "    # Convert numeric features to float\n",
    "    num = ['ua_ph', 'ua_spec_grav', 'age']\n",
    "    for col in num:\n",
    "        mean = df[(df[col] != 'not_reported') & (df[col]!= 'other')][col].astype(\n",
    "            'float').mean()\n",
    "        df[col] = df[col].replace('not_reported', mean)\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df: pd.DataFrame) -> tuple[pd.DataFrame, ColumnTransformer]:\n",
    "    \"\"\"\n",
    "    Input the cleaned dataframe,\n",
    "    OneHotEncode the categorical (non-ordinal) attributes,\n",
    "    OrdinalEncode the ordinal attributes\n",
    "    return the final dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    other = ['ua_ph', 'ua_spec_grav', 'age']\n",
    "    ord = ['ua_blood', 'ua_glucose', 'ua_ketones', 'ua_leuk', 'ua_protein']\n",
    "    onehot = ['chief_complaint', 'race', 'ethnicity',\n",
    "              'maritalStatus', 'employStatus']\n",
    "    label = [i for i in df.columns if i not in ord+other+onehot]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehot', OneHotEncoder(), onehot),\n",
    "            ('label', OrdinalEncoder(), label),\n",
    "            ('ordinal', OrdinalEncoder(categories=[\n",
    "             ['negative', 'small', 'moderate', 'large']]* len(ord)), ord)\n",
    "        ])\n",
    "\n",
    "    transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "    onehot_col_names = preprocessor.named_transformers_[\n",
    "        'onehot'].get_feature_names_out(onehot)\n",
    "    new_column_names = list(onehot_col_names) + label + ord\n",
    "    # Preserve the original index\n",
    "    df_transformed = pd.DataFrame(\n",
    "        transformed, columns=new_column_names, index=df.index)  # type: ignore\n",
    "\n",
    "    df_final = pd.concat([df[other], df_transformed], axis=1)\n",
    "\n",
    "    return df_final, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = trim_missing(df)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, encoder = encode_features(df_cleaned.iloc[:, 2:])\n",
    "Y = df_cleaned.iloc[:, :2]\n",
    "print(f'Feature X shape: {X.shape}')\n",
    "print(f'Label Y shape: {Y.shape}, where'\n",
    "      f'\\n\\tthe first column is true label ({label})'\n",
    "      f'\\n\\tthe second column is ed diagnosis ({diagnosis})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "y_train, y_test = Y_train[label], Y_test[label]\n",
    "\n",
    "assert y_train.name == label\n",
    "assert y_test.name == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train.shape), len(y_val.shape), len(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        assert len(X) == len(y), 'inconsistent shape between X and y'\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "        self.length = len(X)\n",
    "        self.n_feature = X.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        feature = torch.FloatTensor(self.features[i])\n",
    "        label = torch.FloatTensor([self.labels[i]])\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X: np.ndarray):\n",
    "        self.features = X\n",
    "        self.length = len(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        feature = torch.FloatTensor(self.features[i])\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainDataset(X=X_train.values, y=y_train.values)\n",
    "val_data = TrainDataset(X=X_val.values, y=y_val.values)\n",
    "test_data = TestDataset(X=X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           num_workers=DEVICE_N_WORKERS,\n",
    "                                           batch_size=config['batch_size'],\n",
    "                                           pin_memory=True,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data,\n",
    "                                         num_workers=0,\n",
    "                                         batch_size=config['batch_size'],\n",
    "                                         pin_memory=True,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          num_workers=0,\n",
    "                                          batch_size=config['batch_size'],\n",
    "                                          pin_memory=True,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(\n",
    "    train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(\n",
    "    val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(\n",
    "    test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code to check if your data loaders are working\n",
    "for i, (feature, label) in enumerate(train_loader):\n",
    "    print(feature.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, dropout_rate: float):\n",
    "\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(512, 2048),\n",
    "            torch.nn.BatchNorm1d(2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.BatchNorm1d(2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.BatchNorm1d(2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.BatchNorm1d(2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.BatchNorm1d(2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.BatchNorm1d(2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_size=train_data.n_feature,\n",
    "           dropout_rate=config['dropout_rate']).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['init_lr'])\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                       factor=config['scheduler_factor'],\n",
    "                                                       patience=config['scheduler_patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"c3a06f318f071ae7444755a93fa8a5cbff1f6a86\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    name='nn',\n",
    "    reinit=True,  # Allows reinitalizing runs when you re-run this cell\n",
    "    # id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project=\"map\",  # Project should be created in your wandb account\n",
    "    config=config  # Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model architecture as a string with str(model)\n",
    "model_arch = str(model)\n",
    "\n",
    "# Save it in a txt file\n",
    "arch_file = open(\"model_arch.txt\", \"w\")\n",
    "file_write = arch_file.write(model_arch)\n",
    "arch_file.close()\n",
    "\n",
    "# log it in your wandb run with wandb.save()\n",
    "wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    return total_loss, total_acc\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (feature, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        feature = feature.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        # Forward Propagation\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            # print(feature.device)\n",
    "            # print(next(model.parameters()).device)\n",
    "            logits = model(feature)\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # GD\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Record\n",
    "        prediction = (logits >= 0.5).int()\n",
    "        total_loss += loss.item()\n",
    "        total_acc += torch.sum(prediction == label).item() / logits.shape[0]\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(total_acc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        # Release memory\n",
    "        del feature, label, logits, prediction\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    total_acc /= len(dataloader)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    return total_loss, total_acc, precision, recall, f1\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc='Val')\n",
    "\n",
    "    for i, (feature, label) in enumerate(dataloader):\n",
    "        feature = feature.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        # Forward Propagation\n",
    "        with torch.inference_mode():\n",
    "            logits = model(feature)\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "        # Record\n",
    "        prediction = (logits >= 0.5).int()\n",
    "        total_loss += loss.item()\n",
    "        total_acc += torch.sum(prediction == label).item() / logits.shape[0]\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(total_acc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        labels.extend(label.tolist())\n",
    "        predictions.extend(prediction.tolist())\n",
    "\n",
    "        # Release memory\n",
    "        del feature, label, logits, prediction\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    total_acc /= len(dataloader)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return (total_loss, total_acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, feature in enumerate(tqdm(test_loader)):\n",
    "\n",
    "            feature = feature.to(DEVICE)\n",
    "            logits = model(feature)\n",
    "            prediction = (logits >= 0.5).int()\n",
    "            predictions.extend(prediction.tolist())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performace(model, X_train, X_test, y_train, y_test,\n",
    "                     ljust_len=30):\n",
    "    print('Training accuracy: {}'.format(\n",
    "        \"%.4f\" % model.score(X_train, y_train)))\n",
    "\n",
    "    male, female = X_test.gender == 1, X_test.gender == 0\n",
    "    print('Test accuracy:\\n\\t{}{}\\n\\t{}{}\\n\\t{}{}'.format(\n",
    "        'General population'.ljust(ljust_len),\n",
    "        \"%.4f\" % model.score(X_test, y_test),\n",
    "        'Male'.ljust(ljust_len),\n",
    "        \"%.4f\" % model.score(X_test[male], y_test[male]),\n",
    "        'Female'.ljust(ljust_len),\n",
    "        \"%.4f\" % model.score(X_test[female], y_test[female])))\n",
    "\n",
    "    employ_cols = X_test.columns[X_test.columns.str.contains('employStatus')]\n",
    "    for employ_col in employ_cols:\n",
    "        rows = X_test[employ_col] == 1\n",
    "        print('\\t{}{}'.format(\n",
    "            employ_col.split('_')[-1].ljust(ljust_len),\n",
    "            \"%.4f\" % model.score(X_test[rows], y_test[rows])))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print('\\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(file_path, model, optimizer, scaler, scheduler,\n",
    "                    epoch, train_acc, val_acc, precision, recall, f1):\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'scaler_state_dict': scaler.state_dict(),\n",
    "                  'scheduler_state_dict': scheduler.state_dict(),\n",
    "                  'train_accuray': train_acc, 'val_accuray': val_acc,\n",
    "                  'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  57%|█████▋    | 644/1121 [00:33<00:09, 48.99it/s, acc=85.5833%, loss=0.6650]"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc = train(model,\n",
    "                                  train_loader,\n",
    "                                  criterion,\n",
    "                                  optimizer,\n",
    "                                  scaler)\n",
    "    val_loss, val_acc, precision, recall, f1 = eval(model,\n",
    "                                                    val_loader,\n",
    "                                                    criterion)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(\n",
    "        train_acc*100, train_loss, curr_lr))\n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(\n",
    "        val_acc*100, val_loss))\n",
    "    print(\"\\tVal Precison {:.04f}\\tRecall {:.04f}\\tF1 {:.04f}\".format(\n",
    "        precision, recall, f1))\n",
    "\n",
    "    wandb.log({\n",
    "        'lr': curr_lr,\n",
    "        'train_acc': train_acc*100,\n",
    "        'train_loss': train_loss,\n",
    "        'val_acc': val_acc*100,\n",
    "        'val_loss': val_loss,\n",
    "        'val_precison': precision,\n",
    "        'val_recall': recall,\n",
    "        'val_f1': f1\n",
    "    })\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    if (val_acc > best_score):\n",
    "        best_score = val_acc\n",
    "        save_checkpoint(f'{run.id}_best_model.pt', model, optimizer, scaler, scheduler,\n",
    "                        epoch, train_acc, val_acc, precision, recall, f1)\n",
    "        print(f'Best model saved at epoch {epoch}')\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test(model, test_loader)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(f'{wandb.run.id}_best_model.pt',\n",
    "                            model, optimizer, scaler, scheduler,\n",
    "                            epoch, train_acc, val_acc, precision, recall, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
